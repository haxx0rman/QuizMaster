# =============================================================================
# QuizMaster Configuration File
# =============================================================================
# 
# This file contains all configuration options for the QuizMaster system.
# Copy this file to .env and configure your settings.
#
# IMPORTANT NOTES:
# - Remove the .example extension when creating your .env file
# - Never commit your actual .env file to version control (it contains secrets)
# - Boolean values should be: true/false (lowercase)
# - Paths can be absolute or relative to the project root
# - Environment variables override these defaults
#
# =============================================================================

# =============================================================================
# LLM API Configuration
# =============================================================================
# Configure your language model providers. At minimum, you need either OpenAI
# or a local LLM setup. Multiple providers can be configured for redundancy.

# OpenAI Configuration (Primary Provider)
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1
# Optional: Specify organization ID if using OpenAI for Business
OPENAI_ORG_ID=your_org_id_here

# LLM Model Selection
# Recommended models for different use cases:
# - gpt-4o-mini: Cost-effective, good for most tasks
# - gpt-4o: Higher quality, more expensive
# - gpt-3.5-turbo: Fastest, lowest cost
LLM_MODEL=gpt-4o-mini
LLM_MODEL_BACKUP=gpt-3.5-turbo

# Embedding Models (Critical for knowledge graph quality)
# Recommended: text-embedding-3-small (good balance of cost/quality)
# Alternative: text-embedding-3-large (higher quality, more expensive)
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_MODEL_BACKUP=text-embedding-ada-002

# Alternative LLM Providers
# Uncomment and configure if you want to use Anthropic's Claude models
# ANTHROPIC_API_KEY=your_anthropic_key_here
# ANTHROPIC_BASE_URL=https://api.anthropic.com
# ANTHROPIC_MODEL=claude-3-sonnet-20240229

# Local LLM Configuration (e.g., Ollama, vLLM, LocalAI)
# Useful for privacy-sensitive deployments or cost reduction
# Example for Ollama: http://localhost:11434/v1
# LOCAL_LLM_BASE_URL=http://localhost:11434/v1
# LOCAL_LLM_MODEL=llama2
# LOCAL_EMBEDDING_MODEL=nomic-embed-text

# =============================================================================
# Knowledge Extraction Configuration (LightRAG Integration)
# =============================================================================
# These settings control how documents are processed and knowledge graphs are built.
# LightRAG is the core knowledge extraction engine that powers question generation.
# Configuration based on proven patterns from production implementations.

# LightRAG Core Settings
# Directory where LightRAG stores its knowledge base and indices
# This directory will be created automatically if it doesn't exist
LIGHTRAG_WORKING_DIR=./data/lightrag

# Logging level for LightRAG operations
# Options: DEBUG (very verbose), INFO (normal), WARNING (errors only), ERROR (critical only)
LIGHTRAG_LOG_LEVEL=INFO

# Maximum number of concurrent workers for async operations
# Higher values = faster processing but more resource usage
# Recommended range: 2-8 depending on your system and API rate limits
LIGHTRAG_MAX_ASYNC_WORKERS=4

# Document chunking parameters (affects knowledge extraction quality)
# Chunk size: Number of tokens per document chunk
# - Larger chunks (1500+): Better context, may miss fine details
# - Smaller chunks (800-): More precise, may lose broader context
# Recommended: 1200 for educational content
LIGHTRAG_CHUNK_SIZE=1200

# Chunk overlap: Number of tokens that overlap between adjacent chunks
# This helps maintain context continuity across chunk boundaries
# Recommended range: 50-200 tokens
LIGHTRAG_CHUNK_OVERLAP=100

# Whether to reuse existing LightRAG knowledge base
# true: Add new documents to existing knowledge base (incremental)
# false: Create new knowledge base from scratch (full rebuild)
LIGHTRAG_USE_EXISTING=true

# LightRAG LLM Configuration (Based on proven patterns from lightrag_ex.py)
# Maximum token size for LLM model context (affects processing capability)
LLM_MODEL_MAX_TOKEN_SIZE=22192

# Context size for LLM model options (must be â‰¤ max_token_size)
LLM_MODEL_NUM_CTX=20192

# LLM request timeout in seconds (increase for complex processing)
LLM_TIMEOUT=3000

# Number of threads for LLM processing (optimized for performance)
LLM_NUM_THREADS=11

# Host URLs for LLM and embedding services
# Use your Ollama server or compatible API endpoint
LLM_BINDING_HOST=http://localhost:11434
EMBEDDING_BINDING_HOST=http://localhost:11434

# LightRAG Embedding Configuration (Critical for knowledge graph quality)
# Embedding dimension (must match your embedding model)
# Common values: 1024 (bge-m3), 1536 (OpenAI), 768 (sentence-transformers)
EMBEDDING_DIM=1024

# Maximum tokens for embedding processing (affects batch size and memory)
MAX_EMBED_TOKENS=40000

# Embedding request timeout in seconds (embeddings can be slow)
EMBEDDING_TIMEOUT=6000

# Number of threads for embedding processing
EMBEDDING_NUM_THREADS=11

# LightRAG Query Settings (Controls knowledge retrieval behavior)
# Default query mode for knowledge retrieval
# - local: Focuses on context-dependent information (fast, precise)
# - global: Utilizes global knowledge patterns (comprehensive)
# - hybrid: Combines local and global methods (balanced, recommended)
# - mix: Integrates knowledge graph and vector retrieval
# - naive: Basic search without advanced techniques
LIGHTRAG_DEFAULT_MODE=hybrid

# Number of top results to retrieve during queries
# Higher values = more comprehensive results but slower processing
# Recommended range: 20-100 depending on your use case
LIGHTRAG_TOP_K=60

# Maximum tokens for query context (affects LLM input size and API costs)
# This limits how much context is sent to the LLM for each query
# Recommended range: 10000-50000 tokens
LIGHTRAG_MAX_TOKENS=30000

# Vector Storage Configuration (Based on proven patterns)
# Vector storage backend for efficient similarity search
# Options: FaissVectorDBStorage (recommended), NanoVectorDBStorage, ChromaVectorDBStorage
VECTOR_STORAGE=FaissVectorDBStorage

# Cosine similarity threshold for vector storage (affects precision/recall)
# Lower values = more results but potentially less relevant
# Higher values = fewer but more relevant results
# Recommended range: 0.2-0.5
COSINE_BETTER_THAN_THRESHOLD=0.3

# Knowledge Graph Extraction Quality Settings
# These parameters control how entities and relationships are extracted from text
# Fine-tune these based on your content type and quality requirements

# Maximum entities to extract per document chunk
# Higher values = more comprehensive but potentially noisy
# Recommended range: 10-30 entities per chunk
KG_MAX_ENTITIES_PER_CHUNK=20

# Maximum relationships to extract per document chunk
# Should be proportional to entities per chunk
# Recommended range: 10-25 relationships per chunk
KG_MAX_RELATIONSHIPS_PER_CHUNK=15

# Similarity threshold for entity deduplication (0.0-1.0)
# Higher values = stricter matching, fewer duplicates but may miss variations
# Lower values = looser matching, more duplicates but catches variations
# Recommended: 0.7-0.9
KG_ENTITY_SIMILARITY_THRESHOLD=0.8

# Similarity threshold for relationship deduplication (0.0-1.0)
# Similar to entity threshold but for relationships between entities
# Typically slightly lower than entity threshold
KG_RELATIONSHIP_SIMILARITY_THRESHOLD=0.7

# Advanced Storage Configuration (For Power Users)
# These settings allow customization of storage backends
# Most users should leave these at defaults unless you have specific requirements

# Knowledge graph storage backend
# Options: NetworkXStorage (default), Neo4JStorage, PGGraphStorage, MemgraphStorage
# NetworkXStorage: Simple, file-based, good for most use cases
# Neo4JStorage: Enterprise graph database, requires Neo4j installation
# PGGraphStorage: PostgreSQL-based, good for large-scale deployments
KG_STORAGE_TYPE=NetworkXStorage

# Vector database backend
# Options: FaissVectorDBStorage (recommended), NanoVectorDBStorage, MilvusVectorDBStorage, ChromaVectorDBStorage
# FaissVectorDBStorage: Facebook's FAISS, efficient and well-tested
# NanoVectorDBStorage: Lightweight, good for small to medium datasets
# MilvusVectorDBStorage: Enterprise vector database, requires Milvus
# ChromaVectorDBStorage: Modern vector database with good Python integration
VECTOR_STORAGE_TYPE=FaissVectorDBStorage

# Key-value storage backend
# Options: JsonKVStorage (default), PGKVStorage, RedisKVStorage, MongoKVStorage
# JsonKVStorage: Simple file-based storage, good for development and small deployments
# PGKVStorage: PostgreSQL-based, good for production with existing PostgreSQL
# RedisKVStorage: In-memory storage, very fast but requires Redis
# MongoKVStorage: Document database, good for complex document structures
KV_STORAGE_TYPE=JsonKVStorage
LIGHTRAG_CHUNK_SIZE=1200

# Overlap between chunks to maintain context continuity
# Should be 5-15% of chunk size
LIGHTRAG_CHUNK_OVERLAP=100

# Whether to reuse existing LightRAG knowledge base
# true: Load existing data (recommended for production)
# false: Start fresh each time (useful for development)
LIGHTRAG_USE_EXISTING=true

# LightRAG Query Settings
# Default query mode for knowledge retrieval
# Options:
# - local: Fast, context-dependent information
# - global: Comprehensive, broader knowledge
# - hybrid: Balanced approach (recommended)
# - mix: Combined knowledge graph and vector search
LIGHTRAG_DEFAULT_MODE=hybrid

# Number of top results to retrieve for queries
# Higher values = more comprehensive but slower responses
# Recommended range: 20-100
LIGHTRAG_TOP_K=60

# Maximum tokens for query context (affects LLM input size and cost)
# Higher values = more context but higher API costs
# Recommended range: 20000-50000
LIGHTRAG_MAX_TOKENS=30000

# Knowledge Graph Fine-tuning Parameters
# These control the quality vs. quantity tradeoff in knowledge extraction

# Maximum entities to extract per document chunk
# Higher values = more detailed graphs but may include noise
KG_MAX_ENTITIES_PER_CHUNK=20

# Maximum relationships to extract per document chunk
# Should generally be lower than max entities
KG_MAX_RELATIONSHIPS_PER_CHUNK=15

# Similarity thresholds for deduplication (0.0-1.0)
# Higher values = stricter matching, less deduplication
# Lower values = more aggressive deduplication, may merge distinct concepts
KG_ENTITY_SIMILARITY_THRESHOLD=0.8
KG_RELATIONSHIP_SIMILARITY_THRESHOLD=0.7

# Advanced Storage Configuration (for expert users)
# These control which storage backends LightRAG uses
# Default values work well for most use cases

# Knowledge graph storage backend
# Options: NetworkXStorage (default), Neo4JStorage, PGGraphStorage, MemgraphStorage
KG_STORAGE_TYPE=NetworkXStorage

# Vector database for embeddings
# Options: NanoVectorDBStorage (default), PGVectorStorage, MilvusVectorDBStorage, ChromaVectorDBStorage
VECTOR_STORAGE_TYPE=NanoVectorDBStorage

# Key-value storage for caching and metadata
# Options: JsonKVStorage (default), PGKVStorage, RedisKVStorage, MongoKVStorage
KV_STORAGE_TYPE=JsonKVStorage

# =============================================================================
# Question Generation Configuration
# =============================================================================
# These settings control how questions are generated, their difficulty distribution,
# and quality standards. Adjust these based on your learning objectives.

# Core Generation Settings
# LLM model specifically for question generation (can differ from main LLM)
# Using a different model allows optimization for cost vs. quality
QUESTION_GEN_MODEL=gpt-4o-mini

# Temperature controls creativity in question generation (0.0-1.0)
# - 0.0-0.3: Very consistent, deterministic questions
# - 0.4-0.7: Balanced creativity (recommended)
# - 0.8-1.0: High creativity, more varied but potentially inconsistent
QUESTION_GEN_TEMPERATURE=0.7

# Maximum tokens per question generation request
# Higher values allow for more detailed questions and explanations
QUESTION_GEN_MAX_TOKENS=2048

# Number of questions to generate in each batch
# Higher values are more efficient but use more tokens per request
QUESTION_GEN_BATCH_SIZE=5

# Question Type Distribution (must sum to 100%)
# These percentages control the cognitive complexity mix of generated questions

# Single-hop questions: Direct, factual questions from single knowledge sources
# Example: "What is the capital of France?"
SINGLE_HOP_PERCENTAGE=40

# Multi-hop questions: Complex reasoning across multiple knowledge sources  
# Example: "How does climate change in Arctic regions affect global ocean currents?"
MULTI_HOP_PERCENTAGE=30

# Abstract questions: Conceptual, interpretive questions requiring deeper thinking
# Example: "What are the ethical implications of AI decision-making in healthcare?"
ABSTRACT_PERCENTAGE=20

# Specific questions: Precise, detailed questions about particular facts or procedures
# Example: "What is the exact syntax for a SQL LEFT JOIN statement?"
SPECIFIC_PERCENTAGE=10

# Difficulty Level Distribution (must sum to 100%)
# Controls the learning progression and scaffolding of questions

# Beginner: Basic concepts, foundational knowledge
BEGINNER_PERCENTAGE=25

# Intermediate: Application of concepts, moderate complexity
INTERMEDIATE_PERCENTAGE=40

# Advanced: Complex analysis, synthesis of multiple concepts
ADVANCED_PERCENTAGE=25

# Expert: Highest level thinking, evaluation and creation
EXPERT_PERCENTAGE=10

# Quality Assurance Thresholds
# These settings ensure generated questions meet educational standards

# Minimum quality score for generated questions (0.0-1.0)
# Questions below this threshold will be rejected and regenerated
MIN_QUESTION_QUALITY_SCORE=0.7

# Minimum plausibility score for generated answers (0.0-1.0)
# Ensures answers are realistic and educationally valuable
MIN_ANSWER_PLAUSIBILITY_SCORE=0.6

# Maximum retry attempts for failed question generation
# Prevents infinite loops while ensuring quality
MAX_RETRIES_PER_QUESTION=3

# =============================================================================
# Human Learning Adaptation Settings
# =============================================================================
# These settings control adaptive learning features including spaced repetition,
# difficulty adjustment, and personalization systems.

# Spaced Repetition Configuration
# Algorithm for scheduling question reviews based on learning science
# Options: SM2 (SuperMemo 2), Anki (Anki algorithm), Custom
SPACED_REPETITION_ALGORITHM=SM2

# Initial review interval for new questions (in days)
# Newly learned material is reviewed quickly, then intervals increase
INITIAL_INTERVAL_DAYS=1

# Maximum review interval to prevent questions from disappearing forever
# Even well-known material should be reviewed occasionally
MAXIMUM_INTERVAL_DAYS=365

# Ease factor bounds (difficulty adjustment multipliers)
# These control how much the review interval changes based on performance
# Lower ease = more frequent reviews (for difficult material)
# Higher ease = less frequent reviews (for easy material)
EASE_FACTOR_MIN=1.3
EASE_FACTOR_MAX=3.0
EASE_FACTOR_DEFAULT=2.5

# ELO Rating System for Dynamic Difficulty
# Tracks both learner ability and question difficulty using chess-like ratings

# Starting ELO rating for new learners and questions
# Standard chess rating: 1200 = average, 1600 = good, 2000+ = expert
INITIAL_ELO_RATING=1200

# ELO K-factor controls how quickly ratings change
# Higher K-factor = faster adaptation but more volatility
# Lower K-factor = slower adaptation but more stability
# Recommended: 32 for beginners, 16 for experienced learners
ELO_K_FACTOR=32

# ELO rating bounds to prevent extreme values
ELO_MIN_RATING=400
ELO_MAX_RATING=3000

# Learning Personas System
# Enables personalized question generation based on learning styles

# Enable the persona system for individualized learning
# Personas adapt question style, complexity, and presentation
ENABLE_PERSONAS=true

# Maximum number of learning personas per user
# More personas = finer personalization but more complexity
MAX_PERSONAS=5

# LLM model for generating persona-specific content
# Can be different from main model to optimize costs
PERSONA_GENERATION_MODEL=gpt-4o-mini

# =============================================================================
# Question Bank Configuration (qBank Integration)
# =============================================================================
# Settings for managing question storage, organization, and study sessions.

# Storage Settings
# Directory for question bank data files
# This will store your generated questions, learner progress, and analytics
QBANK_DATA_DIR=./data/qbank

# Directory for automatic backups of question banks
# Regular backups protect against data loss
QBANK_BACKUP_DIR=./data/qbank/backups

# Enable automatic backups of question banks
# Recommended: true for production use
AUTO_BACKUP_ENABLED=true

# Hours between automatic backups
# More frequent backups = better protection but more storage usage
BACKUP_INTERVAL_HOURS=24

# Study Session Configuration
# These settings control the learning experience and session structure

# Default number of questions per study session
# Based on learning science: 10-15 questions is optimal for most learners
DEFAULT_SESSION_SIZE=10

# Maximum questions allowed in a single session
# Prevents fatigue and maintains learning effectiveness
MAX_SESSION_SIZE=50

# Suggested session duration in minutes
# Used for pacing recommendations and break suggestions
SUGGESTED_SESSION_MINUTES=30

# Automatically adjust session difficulty based on performance
# true: System adapts to learner ability (recommended)
# false: Manual difficulty selection only
ENABLE_ADAPTIVE_SESSIONS=true

# Performance Tracking and Analytics
# These features help optimize learning and provide insights

# Track how long learners take to answer questions
# Useful for identifying difficult questions and optimizing pacing
TRACK_RESPONSE_TIMES=true

# Monitor learner progression through difficulty levels
# Enables adaptive difficulty and progress visualization
TRACK_DIFFICULTY_PROGRESSION=true

# Enable comprehensive learning analytics and insights
# Provides detailed performance reports and learning recommendations
ENABLE_ANALYTICS=true

# =============================================================================
# System Configuration
# =============================================================================
# Core system settings for logging, performance, caching, and development.

# Logging Configuration
# Controls what information is recorded and where

# Logging verbosity level
# DEBUG: Very detailed (for development), INFO: Normal operation (recommended)
# WARNING: Only errors and warnings, ERROR: Critical errors only
LOG_LEVEL=INFO

# Python logging format string (advanced users only)
# Default format includes timestamp, component name, level, and message
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s

# Path to log file (directory will be created if needed)
# Logs are essential for troubleshooting and monitoring system health
LOG_FILE=./logs/quizmaster.log

# Maximum log file size before rotation (MB)
# Prevents log files from growing too large
MAX_LOG_FILE_SIZE_MB=100

# Number of rotated log files to keep
# Maintains history while controlling disk usage
LOG_BACKUP_COUNT=5

# Performance and API Management
# These settings control resource usage and API rate limiting

# Maximum concurrent API requests
# Prevents overwhelming API providers and hitting rate limits
# Lower values = more conservative, higher values = faster processing
MAX_CONCURRENT_REQUESTS=10

# Timeout for individual API requests (seconds)
# Prevents hanging requests from blocking the system
REQUEST_TIMEOUT_SECONDS=30

# Enable LLM response caching for cost savings
# Caches identical requests to reduce API costs and improve response times
# Recommended: true for production use
CACHE_ENABLED=true

# Cache time-to-live in seconds (1 hour default)
# How long cached responses remain valid
# Longer = more cost savings but potentially stale responses
CACHE_TTL_SECONDS=3600

# Maximum number of cached responses in memory
# Higher values = more cache hits but more memory usage
CACHE_MAX_SIZE=1000

# Development and Debugging Settings
# These features are useful during development and testing

# Enable debug mode with verbose logging and validation checks
# WARNING: Debug mode impacts performance, only use during development
DEBUG_MODE=false

# Enable performance profiling (impacts performance)
# Useful for identifying bottlenecks during development
ENABLE_PROFILING=false

# Use mock responses instead of real LLM calls
# Useful for testing without incurring API costs
# true: No API calls made, uses sample responses
# false: Normal operation with real API calls
MOCK_LLM_RESPONSES=false

# Enable very detailed operation logging
# WARNING: Extremely verbose, only use for deep debugging
ENABLE_DETAILED_LOGGING=false

# =============================================================================
# Export/Import Configuration
# =============================================================================
# Settings for exporting question banks and importing external content.

# Export Settings
# Default format for exporting question banks
# Options: json, csv, markdown, excel
DEFAULT_EXPORT_FORMAT=json

# Enable specific export formats
ENABLE_CSV_EXPORT=true
ENABLE_MARKDOWN_EXPORT=true

# Include metadata (creation dates, performance stats) in exports
# Useful for comprehensive backups but increases file size
INCLUDE_METADATA_IN_EXPORTS=true

# Import Settings
# Automatically detect file format during import
# Supports JSON, CSV, and various text formats
AUTO_DETECT_FORMAT=true

# Validate imported questions for correctness and completeness
# Recommended: true to maintain question bank quality
VALIDATE_IMPORTS=true

# Skip invalid questions during import rather than failing entirely
# Allows partial imports when some questions have issues
SKIP_INVALID_QUESTIONS=true

# Maximum size for import files (MB)
# Prevents memory issues with very large imports
MAX_IMPORT_FILE_SIZE_MB=50

# =============================================================================
# Security and Privacy
# =============================================================================
# Settings for API security, data protection, and privacy compliance.

# API Security
# Enable automatic API key rotation (advanced feature)
# Requires additional setup with key management system
ENABLE_API_KEY_ROTATION=false

# Days between automatic API key rotations
API_KEY_ROTATION_DAYS=30

# Enable rate limiting to prevent abuse
# Recommended: true for production deployments
ENABLE_RATE_LIMITING=true

# Maximum requests per minute per user/IP
# Adjust based on your usage patterns and API limits
RATE_LIMIT_REQUESTS_PER_MINUTE=60

# Data Privacy
# Anonymize user data in logs and analytics
# Important for GDPR compliance and privacy protection
ANONYMIZE_USER_DATA=false

# Data retention period in days
# Automatically remove old user data and logs
DATA_RETENTION_DAYS=365

# Enable GDPR compliance mode
# Adds additional privacy protections and user rights
ENABLE_GDPR_MODE=false

# =============================================================================
# Advanced Features
# =============================================================================
# Experimental and advanced features for power users.

# Multi-language Support
# Default language for generated content
# Supports ISO language codes (en, es, fr, de, etc.)
DEFAULT_LANGUAGE=en

# Enable automatic translation of questions
# Requires additional translation model configuration
ENABLE_TRANSLATION=false

# LLM model for translation tasks
TRANSLATION_MODEL=gpt-4o-mini

# Content Filtering
# Enable automatic content filtering for inappropriate material
# Recommended: true for educational environments
ENABLE_CONTENT_FILTERING=true

# Content filter strictness level
# Options: low, medium, high
CONTENT_FILTER_STRICTNESS=medium

# Block questions containing inappropriate content
BLOCK_INAPPROPRIATE_CONTENT=true

# Experimental Features (use with caution)
# These features are under development and may be unstable

# Enable all experimental features
# WARNING: May cause unexpected behavior
ENABLE_EXPERIMENTAL_FEATURES=false

# Automatically refine questions based on learner feedback
# Uses ML to improve question quality over time
ENABLE_AUTO_QUESTION_REFINEMENT=false

# Enable collaborative learning features
# Allows learners to share and rate questions
ENABLE_COLLABORATIVE_LEARNING=false

# =============================================================================
# Configuration Complete
# =============================================================================
# 
# After configuring this file:
# 1. Save it as .env (remove the .example extension)
# 2. Ensure your API keys are valid and have sufficient credits
# 3. Test your configuration with: python examples/lightrag_integration_demo.py
# 4. Check the logs directory for any configuration warnings
# 
# For support and documentation, visit:
# - GitHub Repository: [Add your repo URL here]
# - Documentation: [Add your docs URL here]
# 
# =============================================================================
